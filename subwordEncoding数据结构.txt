data  -> 
	#Alphabet类
	word_alphabet    = Alphabet('word')		#放入每行 中的字
	 biword_alphabet  =Alphabet('biword')		#放入每行字+下一行字组成的bigram
	 char_alphabet   =Alphabet('character')		#放入每行 中的字
	 label_alphabet  = Alphabet('label', True)     # 放入每行字的标签 B-SEG/E-SEG...
	 gaz_alphabet = Alphabet('gaz')			#训练集等在gaz_file中匹配到的词
	
	#普通变量
	word_alphabet_size、biword_alphabet_size...
	tagScheme    # "BMES" or  "BIO"

	#Gazetteer类
	gaz = Gazetteer(self.gaz_lower)
	gaz_alphabet = Alphabet('gaz')    

	#数据集
	self.train_texts = []	self.dev_texts = []	self.test_texts = []	self.raw_texts = []
        self.train_Ids = []	self.dev_Ids = []	self.test_Ids = []	self.raw_Ids = []

        self.pretrain_word_embedding = None
        self.pretrain_biword_embedding = None
        self.pretrain_gaz_embedding = None

Alphabet  ->      __name
		instance2index = {}		#加入训练集匹配到的单词  +  index    instance2index[0]="</unk>"
		instances = []			#加入训练集匹配到的单词

Gazetteer   ->   trie = Trie() 
		 ent2type = {}				# word list to type   
		 ent2id = {"<UNK>":0}			# word to id
		 space = ""				#始终为空  用于space.join(word_list)  :list 就变成了字符串

Trie     ->    root = TrieNode()
TrieNode ->    children = collections.defaultdict(TrieNode)




main.py:
	model = BiLSTM_CRF(data)


bilstmcrf.py:
	self.lstm = BiLSTM(data)
	self.crf = CRF(label_size, self.gpu)

bilstm.py:
	self.char_feature = CharBiLSTM(data.char_alphabet.size(), self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)
	self.word_embeddings = copy_(pretrain_word_embedding)   [word_alphabet.size() , embedding_dim]
	self.biword_embeddings = copy_(pretrain_biword_embedding)
	lstm_input = self.embedding_dim + self.char_hidden_dim + data.biword_emb_dim

	self.forward_lstm = LatticeLSTM()      #网格LSTM
	self.backward_lstm

	hidden2tag   #线性变换层？

crf.py:
	self.transitions = nn.Parameter(init_transitions) = torch.zeros(self.tagset_size+2, self.tagset_size+2)	
	
charbilstm.py:
	self.hidden_dim
	self.char_drop=nn.Dropout(dropout) #dropout=0.5
	self.char_embeddings   #均匀分布中抽取样本 ??????????
	self.char_lstm= nn.LSTM()


latticelstm.py:							 MultiInputLSTMCell()   WordLSTMCell()
LatticeLSTM()：
	self.word_emb = copy_(pretrain_word_emb)
        self.rnn = MultiInputLSTMCell(input_dim, hidden_dim)
        self.word_rnn = WordLSTMCell(word_emb_dim, hidden_dim)

WordLSTMCell()
	self.weight_ih = nn.Parameter(torch.FloatTensor(input_size, 3 * hidden_size))  
	self.weight_hh = nn.Parameter(torch.FloatTensor(hidden_size, 3 * hidden_size))
	self.bias = nn.Parameter(torch.FloatTensor(3 * hidden_size))

MultiInputLSTMCell()
	self.weight_ih = nn.Parameter(torch.FloatTensor(input_size, 3 * hidden_size))
        self.weight_hh = nn.Parameter(torch.FloatTensor(hidden_size, 3 * hidden_size))
	self.alpha_weight_ih = nn.Parameter(torch.FloatTensor(input_size, hidden_size))
	self.alpha_weight_hh = nn.Parameter(torch.FloatTensor(hidden_size, hidden_size))
	self.bias = nn.Parameter(torch.FloatTensor(3 * hidden_size))
	self.alpha_bias = nn.Parameter(torch.FloatTensor(hidden_size))


























